{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='BLACK'>Markov Decision Processes (MDPs)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markov decision processes__ give us a formal way for sequential decision making.\n",
    "\n",
    "MDP Consit of:\n",
    "\n",
    "1. __Environment__\n",
    "2. __Agent__\n",
    "3. __State__\n",
    "4. __Actions__\n",
    "5. __Rewards__\n",
    "\n",
    "This process of selecting an action from a given state, transitioning to a new state, and receiving a reward happens sequentially over and over again, which creates something called a trajectory that shows the sequence of states, actions, and rewards.\n",
    "\n",
    "Throughout this process, it is the agent’s goal to maximize the total amount of rewards that it receives from taking actions in given states. This means that the agent wants to maximize not just the immediate reward, but the cumulative rewards it receives over time.\n",
    "\n",
    "It is the agent’s goal to maximize the cumulative rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP notation\n",
    "We’re now going to repeat what we just  discussed above.\n",
    "\n",
    "In an MDP, we have a set of all possible states \n",
    "__S__\n",
    ", a set of all possible actions **A** that can be taken in a <font color='blue'>episode</font>(i.e For example, in a car racing video game, you start the game (initial state) and play the game until it is over (final state). This is called an episode. Once the game is over, you start the next episode by restarting the game, and you will begin from the initial state irrespective of the position you were in the previous game. So, each episode is independent of the other.)  \n",
    "\n",
    ", and a set of rewards(i.e diffrent value of rewards that can be awarded) \n",
    "**R**\n",
    ". We assume that each of these sets has a finite number of elements.\n",
    "\n",
    "At each time step \n",
    "t\n",
    "=\n",
    "0\n",
    ",\n",
    "1\n",
    ",\n",
    "2\n",
    ",\n",
    "⋯\n",
    ", the agent receives some representation of the environment’s state \n",
    "S\n",
    "t\n",
    "∈\n",
    "S\n",
    ". Based on this state, the agent selects an action \n",
    "A\n",
    "t\n",
    "∈\n",
    "A\n",
    ". This gives us the state-action pair \n",
    "(\n",
    "S\n",
    "t\n",
    ",\n",
    "A\n",
    "t\n",
    ")\n",
    ".\n",
    "\n",
    "Time is then incremented to the next time step \n",
    "t\n",
    "+\n",
    "1\n",
    ", and the environment is transitioned to a new state \n",
    "S\n",
    "t\n",
    "+\n",
    "1\n",
    "∈\n",
    "S\n",
    ". At this time, the agent receives a numerical reward \n",
    "R\n",
    "t\n",
    "+\n",
    "1\n",
    "∈\n",
    "R\n",
    " for the action \n",
    "A\n",
    "t\n",
    " taken from state \n",
    "S\n",
    "t\n",
    ".\n",
    "\n",
    "We can think of the process of receiving a reward as an arbitrary function \n",
    "f\n",
    " that maps state-action pairs to rewards. At each time \n",
    "t\n",
    ", we have\n",
    "f\n",
    "(\n",
    "S\n",
    "t\n",
    ",\n",
    "A\n",
    "t\n",
    ")\n",
    "=\n",
    "R\n",
    "t\n",
    "+\n",
    "1\n",
    ".\n",
    "The trajectory representing the sequential process of selecting an action from a state, transitioning to a new state, and receiving a reward can be represented as\n",
    "S\n",
    "0\n",
    ",\n",
    "A\n",
    "0\n",
    ",\n",
    "R\n",
    "1\n",
    ",\n",
    "S\n",
    "1\n",
    ",\n",
    "A\n",
    "1\n",
    ",\n",
    "R\n",
    "2\n",
    ",\n",
    "S\n",
    "2\n",
    ",\n",
    "A\n",
    "2\n",
    ",\n",
    "R\n",
    "3\n",
    ",\n",
    "⋯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/MDP-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "1. At time t, the environment is in state St.\n",
    "\n",
    "2. The agent observes the current state and selects action \n",
    "A\n",
    "t\n",
    ".\n",
    "\n",
    "3. The environment transitions to state \n",
    "S\n",
    "t\n",
    "+\n",
    "1\n",
    " and grants the agent reward \n",
    "R\n",
    "t\n",
    "+\n",
    "1\n",
    ".\n",
    "\n",
    "4. This process then starts over for the next time step, \n",
    "t\n",
    "+\n",
    "1\n",
    ".\n",
    "Note, \n",
    "t\n",
    "+\n",
    "1\n",
    " is no longer in the future, but is now the present. When we cross the dotted line on the bottom left, the diagram shows \n",
    "t\n",
    "+\n",
    "1\n",
    " transforming into the current time step \n",
    "t\n",
    " so that \n",
    "S\n",
    "t\n",
    "+\n",
    "1\n",
    " and \n",
    "R\n",
    "t\n",
    "+\n",
    "1\n",
    " are now \n",
    "S\n",
    "t\n",
    " and \n",
    "R\n",
    "t\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In ReinforcementLearning we have 2 types of tasks:-\n",
    "1. EPISODIC TASKS\n",
    "2. CONTINOUS TASKS\n",
    "\n",
    "\n",
    "For example, in a car racing video game, you start the game (initial state) and play the game until it is over (final state). This is called an episode. Once the game is over, you start the next episode by restarting the game, and you will begin from the initial state irrespective of the position you were in the previous game. So, each episode is independent of the other.\n",
    "\n",
    "\n",
    "In a continuous task, there is not a terminal state. Continuous tasks will never end. For example, a personal assistance robot does not have a terminal state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences\n",
    "http://deeplizard.com/learn/video/my207WNoeyA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
