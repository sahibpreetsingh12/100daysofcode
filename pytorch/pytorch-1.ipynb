{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()# super().__init__()  is equal to  super(Net,self).__init__() where self is the object of \n",
    "        #child class which is used to access proxy objectof Parent class created by super method .\n",
    "        \n",
    "        \n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        #Default value of stride is (1,1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120,bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84,bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10,bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=Net()\n",
    "print(net)#this is printed because we have imported Module class of nn package and in this class \n",
    "# __repr__() method is overwritten in this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "<class '__main__.Net'>\n"
     ]
    }
   ],
   "source": [
    "print(type(net.conv1))\n",
    "print(type(net))\n",
    "\n",
    "#this shows how we have accessed conv1 object using net object of Net class\n",
    "#conv1 is object we defined in our __init__ function is instance of torch.nn.modules.conv.Conv2d class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=400, out_features=120, bias=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=120, out_features=84, bias=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0777, -0.1945,  0.1544, -0.1183,  0.0964],\n",
       "          [ 0.0532,  0.0028, -0.1555,  0.1976,  0.1119],\n",
       "          [ 0.0048,  0.0737,  0.1175, -0.1454,  0.0146],\n",
       "          [ 0.1663, -0.0418, -0.1756, -0.1280, -0.1954],\n",
       "          [ 0.0870,  0.1991, -0.0780,  0.1761, -0.1773]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1472, -0.0942, -0.0782, -0.0236,  0.1453],\n",
       "          [ 0.0703, -0.1771, -0.1546,  0.0647,  0.0745],\n",
       "          [ 0.1441, -0.0775, -0.0960,  0.0543, -0.1330],\n",
       "          [ 0.1436,  0.0533, -0.1132, -0.1508,  0.0667],\n",
       "          [-0.0324,  0.0884, -0.0381,  0.1769, -0.0810]]],\n",
       "\n",
       "\n",
       "        [[[-0.1967,  0.0737,  0.0730,  0.1705, -0.1281],\n",
       "          [-0.0587, -0.0366,  0.1587,  0.0676, -0.1368],\n",
       "          [-0.0545,  0.1578, -0.1564, -0.1864, -0.0431],\n",
       "          [ 0.1308, -0.1159,  0.1129,  0.1110,  0.0157],\n",
       "          [ 0.1385,  0.1508,  0.1869, -0.0656,  0.0457]]],\n",
       "\n",
       "\n",
       "        [[[-0.0945, -0.0274, -0.0213, -0.0197,  0.1984],\n",
       "          [ 0.0754, -0.0368,  0.1723,  0.1376,  0.1327],\n",
       "          [ 0.0520,  0.1324, -0.1257,  0.1472,  0.1043],\n",
       "          [-0.1758,  0.1368, -0.0731, -0.0274,  0.0664],\n",
       "          [-0.0223,  0.0988,  0.0911, -0.0003, -0.1511]]],\n",
       "\n",
       "\n",
       "        [[[-0.1089, -0.0669,  0.1533, -0.1222, -0.0415],\n",
       "          [ 0.1121,  0.1509, -0.0409, -0.0307, -0.1136],\n",
       "          [ 0.0088,  0.1489, -0.0515,  0.0736,  0.1366],\n",
       "          [ 0.1872, -0.1432,  0.0764, -0.1701,  0.1553],\n",
       "          [-0.1708,  0.0089, -0.1431, -0.1863,  0.0311]]],\n",
       "\n",
       "\n",
       "        [[[-0.0822,  0.0978, -0.1573, -0.0620, -0.1968],\n",
       "          [-0.0236, -0.1924, -0.0501,  0.1039,  0.1118],\n",
       "          [ 0.0123,  0.0742, -0.0179,  0.1115,  0.0141],\n",
       "          [ 0.1840,  0.0382, -0.1937, -0.1191, -0.1036],\n",
       "          [-0.0045, -0.0089, -0.0997, -0.1731,  0.0638]]]], requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.weight\n",
    "#this shows how we can access weight object inside conv1 object \n",
    "\n",
    "#In output Below Parameter containing: means \n",
    "#the tensor printed is a special tensor which will learn the parameter values as we train our model\n",
    "#adn will try to achieve value in suchway that loss function is minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5]) \n",
      "\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net.conv1.weight.shape,'\\n')\n",
    "for i in range(net.conv1.weight.shape[0]):\n",
    "    print(net.conv1.weight[i].shape)#accessing a specific filter from six filters\n",
    "type(net.conv1.weight.shape)\n",
    "#output shows number of output channels,no of input channels,height of kernel,width of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.conv1.weight)\n",
    "#this shows that weight attribute belongs to Parameter class which is accessed by conv1 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "param=list(net.parameters())\n",
    "print(len(param))\n",
    "for param in net.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This shows that although we have defined :-\n",
    "## 5 layers(2 convolution layers,3 fully connected layers) \n",
    "## We have got 10 parameters because \n",
    "## By default for each layer we have Weight tensor and Bias Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight  SHAPE --->  torch.Size([6, 1, 5, 5]) \n",
      "\n",
      "conv1.bias  SHAPE --->  torch.Size([6]) \n",
      "\n",
      "conv2.weight  SHAPE --->  torch.Size([16, 6, 5, 5]) \n",
      "\n",
      "conv2.bias  SHAPE --->  torch.Size([16]) \n",
      "\n",
      "fc1.weight  SHAPE --->  torch.Size([120, 400]) \n",
      "\n",
      "fc1.bias  SHAPE --->  torch.Size([120]) \n",
      "\n",
      "fc2.weight  SHAPE --->  torch.Size([84, 120]) \n",
      "\n",
      "fc2.bias  SHAPE --->  torch.Size([84]) \n",
      "\n",
      "fc3.weight  SHAPE --->  torch.Size([10, 84]) \n",
      "\n",
      "fc3.bias  SHAPE --->  torch.Size([10]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name , param in net.named_parameters():\n",
    "    print(name ,\" SHAPE ---> \", param.shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing use of RELU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9573, -0.4994, -1.2536])\n",
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "input=torch.randn(3)\n",
    "print(input)\n",
    "m = nn.functional.relu(input)\n",
    "print(m)\n",
    "\n",
    "#Rectifying Negative Input\n",
    "#This also shows Relu Does not Reduce the size of Input  tensor given to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([1, 1, 32, 32]) \n",
      "\n",
      "torch.Size([1, 6, 28, 28]) \n",
      "\n",
      "torch.Size([1, 6, 14, 14]) \n",
      "\n",
      "torch.Size([1, 16, 5, 5]) \n",
      "\n",
      "torch.Size([1, 400]) \n",
      "\n",
      "torch.Size([1, 120]) \n",
      "\n",
      "torch.Size([1, 84]) \n",
      "\n",
      "torch.Size([1, 10]) \n",
      "\n",
      "tensor([[ 0.0592, -0.0786,  0.0614, -0.0773, -0.0693, -0.1795,  0.1136,  0.0597,\n",
      "          0.1579, -0.0359]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        #Activation Functions do not reduce the size of Filter Map Given to Them as Input\n",
    "        relu_layer_1=F.relu(self.conv1(x))# Activation Relu is applied on First Convolutional Layer(conv1)\n",
    "        print(relu_layer_1.size(),'\\n')#Size has reduced from 32,32 in input to 28,28 after this RELU operation \n",
    "        #beacuse Input is First passed from convolution Layer then Relu Layer\n",
    "        \n",
    "\n",
    "\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(relu_layer_1, (2, 2))# Max Pooling is applied on output of Previous Relu Layer using 2,2 Max Pooling\n",
    "        # If the size is a square you can only specify a single number\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        print(x.size(),'\\n')\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()#Setting Gradient to Zero to Deal with Problem of Accumulating Gradient\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1848, -0.7626,  2.0601, -0.3574, -0.2599, -0.2325,  0.7191, -0.1894,\n",
      "         0.6062, -0.1902])\n",
      "tensor([[-0.1848, -0.7626,  2.0601, -0.3574, -0.2599, -0.2325,  0.7191, -0.1894,\n",
      "          0.6062, -0.1902]])\n",
      "<class 'torch.nn.modules.loss.MSELoss'>\n",
      "tensor(0.6049, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#output = net(input)\n",
    "\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "print(target)\n",
    "\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "print(target)\n",
    "criterion = nn.MSELoss()\n",
    "print(type(criterion))\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f5f8a414b00>\n",
      "<AddmmBackward object at 0x7f5f8a4142b0>\n",
      "<AccumulateGrad object at 0x7f5f8a414b00>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Print whole Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <MseLossBackward object at 0x7f5f8a414a20> \n",
      "\n",
      "**** <AddmmBackward object at 0x7f5f88380b00> \n",
      "\n",
      "******** <AccumulateGrad object at 0x7f5f88380588> \n",
      "\n",
      "******** <ReluBackward0 object at 0x7f5f883809e8> \n",
      "\n",
      "************ <AddmmBackward object at 0x7f5f88380780> \n",
      "\n",
      "**************** <AccumulateGrad object at 0x7f5f88380908> \n",
      "\n",
      "**************** <ReluBackward0 object at 0x7f5f88380cc0> \n",
      "\n",
      "******************** <AddmmBackward object at 0x7f5f88380dd8> \n",
      "\n",
      "************************ <AccumulateGrad object at 0x7f5f88380e48> \n",
      "\n",
      "************************ <ViewBackward object at 0x7f5f88380160> \n",
      "\n",
      "**************************** <MaxPool2DWithIndicesBackward object at 0x7f5f88380978> \n",
      "\n",
      "******************************** <ReluBackward0 object at 0x7f5fc4259668> \n",
      "\n",
      "************************************ <MkldnnConvolutionBackward object at 0x7f5fc42599e8> \n",
      "\n",
      "**************************************** <MaxPool2DWithIndicesBackward object at 0x7f5fc4259c18> \n",
      "\n",
      "******************************************** <ReluBackward0 object at 0x7f5f88361668> \n",
      "\n",
      "************************************************ <MkldnnConvolutionBackward object at 0x7f5f883619e8> \n",
      "\n",
      "**************************************************** <AccumulateGrad object at 0x7f5f883617f0> \n",
      "\n",
      "**************************************************** <AccumulateGrad object at 0x7f5f88361978> \n",
      "\n",
      "**************************************** <AccumulateGrad object at 0x7f5fc4259588> \n",
      "\n",
      "**************************************** <AccumulateGrad object at 0x7f5f883617b8> \n",
      "\n",
      "************************ <TBackward object at 0x7f5f883802b0> \n",
      "\n",
      "**************************** <AccumulateGrad object at 0x7f5f88380978> \n",
      "\n",
      "**************** <TBackward object at 0x7f5f883808d0> \n",
      "\n",
      "******************** <AccumulateGrad object at 0x7f5f88380dd8> \n",
      "\n",
      "******** <TBackward object at 0x7f5f88380898> \n",
      "\n",
      "************ <AccumulateGrad object at 0x7f5f88380780> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_graph(g, level=0):\n",
    "    if g == None: return\n",
    "    print('*'*level*4, g,'\\n')\n",
    "    for subg in g.next_functions:\n",
    "        print_graph(subg[0], level+1)\n",
    "\n",
    "print_graph(loss.grad_fn, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete example is Explained Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[-0.0272,  0.0164, -0.1269,  0.0983,  0.0991, -0.0832, -0.0999, -0.0658,\n",
      "          0.0328, -0.0606]], grad_fn=<AddmmBackward>)\n",
      "tensor(1.7300, grad_fn=<MseLossBackward>) \n",
      "\n",
      "When we print the net.conv1.weight.grad we get 6 arrays of size (5,5) this clears that we have given \n",
      "one input image and (5,5) kernel and (5,5) kernel means this kernel of height 5 and width 5 \n",
      "will traverse the image in 25 steps and we want 6 of them so we are getting 6 arrays of (5,5) as output\n",
      " \n",
      "\n",
      "conv1.weight.grad before backward\n",
      "tensor([[[[ 4.3141e-02,  3.1216e-02,  1.9751e-02,  3.5670e-02,  1.1059e-02],\n",
      "          [ 4.8656e-05, -6.7129e-03, -2.8760e-02, -2.9703e-02, -2.1257e-02],\n",
      "          [ 7.0798e-02, -2.1195e-02,  3.0593e-02,  9.2125e-03,  1.5234e-02],\n",
      "          [ 5.9482e-02,  2.2897e-02,  4.8568e-02,  5.8541e-02,  3.9010e-02],\n",
      "          [ 4.7316e-02,  1.1614e-02,  4.5134e-03,  1.7232e-02, -1.0466e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.3576e-03, -5.0543e-02,  3.4153e-03, -1.6936e-02,  8.7316e-02],\n",
      "          [ 4.9568e-02,  3.0811e-02, -1.8968e-02, -1.8183e-02, -1.2468e-03],\n",
      "          [ 2.7507e-02,  6.0770e-02,  1.2743e-02, -1.9761e-02, -1.0642e-01],\n",
      "          [ 2.1241e-02, -5.3657e-02,  3.4015e-02,  7.8466e-02,  6.0489e-02],\n",
      "          [ 8.7584e-03,  3.5469e-02,  3.7991e-02, -1.7474e-02,  5.7721e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1885e-02,  4.3378e-02, -7.0273e-02, -3.4027e-02, -5.7850e-02],\n",
      "          [ 8.7609e-03, -6.5430e-02,  3.6381e-02, -8.4351e-02,  1.8899e-02],\n",
      "          [ 5.2375e-02, -2.9046e-02, -4.3635e-02,  2.8532e-02,  7.2283e-02],\n",
      "          [-2.9823e-02,  2.0401e-02, -1.0487e-02, -6.5160e-03, -1.5259e-02],\n",
      "          [ 2.0485e-02, -6.5696e-02,  4.7042e-02, -7.6087e-02,  3.0225e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.3318e-02, -1.0195e-01,  2.1997e-02, -6.4842e-02, -2.8351e-02],\n",
      "          [-8.1077e-02,  3.8367e-03,  6.7289e-02,  2.8557e-02, -4.9822e-02],\n",
      "          [-1.0559e-02,  1.0981e-02,  1.8406e-02,  3.2233e-02, -2.7207e-02],\n",
      "          [ 2.7368e-02, -2.5880e-03,  6.7123e-02,  3.8611e-02, -2.6560e-02],\n",
      "          [ 1.0958e-01,  5.5106e-03, -3.4050e-02, -9.9453e-03, -1.0124e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8358e-02, -2.2702e-02,  3.2738e-02, -1.7301e-02, -8.0912e-03],\n",
      "          [ 2.1056e-02, -1.4504e-01,  8.2157e-02,  4.1579e-02, -7.5055e-04],\n",
      "          [-5.6562e-02,  2.5567e-02, -9.7457e-02,  3.2811e-03,  2.0035e-03],\n",
      "          [-9.2177e-02,  1.9445e-02,  4.4514e-02,  8.2646e-02, -2.6805e-02],\n",
      "          [-1.3148e-02,  8.7645e-02,  1.9015e-02,  1.6323e-02, -6.8221e-03]]],\n",
      "\n",
      "\n",
      "        [[[-5.4767e-02, -3.5047e-02,  2.6960e-02,  3.9631e-03, -5.8619e-02],\n",
      "          [-3.3230e-02,  2.1464e-02, -2.0775e-03,  1.9583e-02,  9.4922e-02],\n",
      "          [-3.3144e-02,  5.3791e-02, -2.4831e-02, -6.3593e-02,  9.2806e-02],\n",
      "          [ 3.6410e-02,  4.0229e-02,  1.3113e-03, -4.0060e-02,  1.3885e-02],\n",
      "          [ 7.7318e-03, -8.9053e-03, -1.4026e-02,  1.1385e-01, -5.8069e-02]]]]) \n",
      "\n",
      "torch.Size([6, 1, 5, 5]) \n",
      "\n",
      "conv1.weight.grad after backward\n",
      "tensor([[[[ 0.0544,  0.0368,  0.0284,  0.0433,  0.0144],\n",
      "          [-0.0032, -0.0185, -0.0275, -0.0048, -0.0072],\n",
      "          [ 0.0741, -0.0431,  0.0497,  0.0019,  0.0232],\n",
      "          [ 0.0540,  0.0136,  0.0619,  0.0720,  0.0349],\n",
      "          [ 0.0563,  0.0059, -0.0006,  0.0101, -0.0184]]],\n",
      "\n",
      "\n",
      "        [[[-0.0026, -0.0591,  0.0008, -0.0285,  0.1127],\n",
      "          [ 0.0595,  0.0312, -0.0284, -0.0480,  0.0202],\n",
      "          [ 0.0357,  0.0656,  0.0037, -0.0018, -0.1155],\n",
      "          [ 0.0070, -0.0572,  0.0386,  0.0759,  0.0893],\n",
      "          [ 0.0107,  0.0403,  0.0410, -0.0261,  0.0412]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0008,  0.0561, -0.1083, -0.0312, -0.0524],\n",
      "          [ 0.0085, -0.0596,  0.0425, -0.0999,  0.0429],\n",
      "          [ 0.0691, -0.0310, -0.0580,  0.0124,  0.0751],\n",
      "          [-0.0535,  0.0048, -0.0064,  0.0049,  0.0062],\n",
      "          [ 0.0263, -0.0630,  0.0477, -0.0914,  0.0361]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0289, -0.1104,  0.0237, -0.0938, -0.0322],\n",
      "          [-0.0858,  0.0042,  0.0538,  0.0341, -0.0576],\n",
      "          [-0.0203,  0.0257,  0.0261,  0.0322, -0.0181],\n",
      "          [ 0.0258, -0.0042,  0.0588,  0.0439, -0.0409],\n",
      "          [ 0.1053,  0.0069, -0.0393, -0.0088, -0.1072]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0296,  0.0029,  0.0448, -0.0356, -0.0261],\n",
      "          [ 0.0355, -0.1591,  0.0888,  0.0526,  0.0032],\n",
      "          [-0.0713,  0.0176, -0.0844,  0.0051, -0.0071],\n",
      "          [-0.0913,  0.0038,  0.0216,  0.0925, -0.0287],\n",
      "          [-0.0220,  0.1030,  0.0156,  0.0179, -0.0044]]],\n",
      "\n",
      "\n",
      "        [[[-0.0596, -0.0134,  0.0137, -0.0203, -0.0718],\n",
      "          [-0.0443,  0.0280,  0.0010,  0.0293,  0.1146],\n",
      "          [-0.0376,  0.0524, -0.0343, -0.0522,  0.0686],\n",
      "          [ 0.0230,  0.0377,  0.0015, -0.0328,  0.0083],\n",
      "          [-0.0150, -0.0313, -0.0205,  0.1154, -0.0581]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)\n",
    "\n",
    "\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n",
    "\n",
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When we print the net.conv1.weight.grad we get 6 arrays of size (5,5) this clears that we have given one input image and (5,5) kernel and (5,5) kernel means this kernel of height 5 and width 5 will traverse the image in 25 steps and we want 6 of them so we are getting 6 arrays of (5,5) as output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight.grad before backward\n",
      "tensor([[[[ 0.0544,  0.0368,  0.0284,  0.0433,  0.0144],\n",
      "          [-0.0032, -0.0185, -0.0275, -0.0048, -0.0072],\n",
      "          [ 0.0741, -0.0431,  0.0497,  0.0019,  0.0232],\n",
      "          [ 0.0540,  0.0136,  0.0619,  0.0720,  0.0349],\n",
      "          [ 0.0563,  0.0059, -0.0006,  0.0101, -0.0184]]],\n",
      "\n",
      "\n",
      "        [[[-0.0026, -0.0591,  0.0008, -0.0285,  0.1127],\n",
      "          [ 0.0595,  0.0312, -0.0284, -0.0480,  0.0202],\n",
      "          [ 0.0357,  0.0656,  0.0037, -0.0018, -0.1155],\n",
      "          [ 0.0070, -0.0572,  0.0386,  0.0759,  0.0893],\n",
      "          [ 0.0107,  0.0403,  0.0410, -0.0261,  0.0412]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0008,  0.0561, -0.1083, -0.0312, -0.0524],\n",
      "          [ 0.0085, -0.0596,  0.0425, -0.0999,  0.0429],\n",
      "          [ 0.0691, -0.0310, -0.0580,  0.0124,  0.0751],\n",
      "          [-0.0535,  0.0048, -0.0064,  0.0049,  0.0062],\n",
      "          [ 0.0263, -0.0630,  0.0477, -0.0914,  0.0361]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0289, -0.1104,  0.0237, -0.0938, -0.0322],\n",
      "          [-0.0858,  0.0042,  0.0538,  0.0341, -0.0576],\n",
      "          [-0.0203,  0.0257,  0.0261,  0.0322, -0.0181],\n",
      "          [ 0.0258, -0.0042,  0.0588,  0.0439, -0.0409],\n",
      "          [ 0.1053,  0.0069, -0.0393, -0.0088, -0.1072]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0296,  0.0029,  0.0448, -0.0356, -0.0261],\n",
      "          [ 0.0355, -0.1591,  0.0888,  0.0526,  0.0032],\n",
      "          [-0.0713,  0.0176, -0.0844,  0.0051, -0.0071],\n",
      "          [-0.0913,  0.0038,  0.0216,  0.0925, -0.0287],\n",
      "          [-0.0220,  0.1030,  0.0156,  0.0179, -0.0044]]],\n",
      "\n",
      "\n",
      "        [[[-0.0596, -0.0134,  0.0137, -0.0203, -0.0718],\n",
      "          [-0.0443,  0.0280,  0.0010,  0.0293,  0.1146],\n",
      "          [-0.0376,  0.0524, -0.0343, -0.0522,  0.0686],\n",
      "          [ 0.0230,  0.0377,  0.0015, -0.0328,  0.0083],\n",
      "          [-0.0150, -0.0313, -0.0205,  0.1154, -0.0581]]]]) \n",
      "\n",
      "torch.Size([6, 1, 5, 5]) \n",
      "\n",
      "conv1.weight.grad after backward\n",
      "tensor([[[[ 0.0656,  0.0424,  0.0370,  0.0510,  0.0177],\n",
      "          [-0.0064, -0.0303, -0.0263,  0.0201,  0.0068],\n",
      "          [ 0.0775, -0.0651,  0.0688, -0.0054,  0.0312],\n",
      "          [ 0.0486,  0.0043,  0.0752,  0.0854,  0.0308],\n",
      "          [ 0.0652,  0.0002, -0.0058,  0.0030, -0.0263]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0012, -0.0676, -0.0019, -0.0401,  0.1382],\n",
      "          [ 0.0694,  0.0315, -0.0379, -0.0778,  0.0417],\n",
      "          [ 0.0439,  0.0704, -0.0053,  0.0162, -0.1246],\n",
      "          [-0.0072, -0.0608,  0.0431,  0.0733,  0.1182],\n",
      "          [ 0.0126,  0.0451,  0.0440, -0.0346,  0.0247]]],\n",
      "\n",
      "\n",
      "        [[[-0.0104,  0.0688, -0.1463, -0.0284, -0.0469],\n",
      "          [ 0.0082, -0.0537,  0.0487, -0.1155,  0.0669],\n",
      "          [ 0.0857, -0.0330, -0.0724, -0.0038,  0.0779],\n",
      "          [-0.0771, -0.0107, -0.0023,  0.0163,  0.0276],\n",
      "          [ 0.0321, -0.0603,  0.0484, -0.1068,  0.0420]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0246, -0.1189,  0.0254, -0.1228, -0.0361],\n",
      "          [-0.0906,  0.0045,  0.0403,  0.0396, -0.0654],\n",
      "          [-0.0301,  0.0405,  0.0338,  0.0322, -0.0090],\n",
      "          [ 0.0243, -0.0057,  0.0504,  0.0491, -0.0553],\n",
      "          [ 0.1010,  0.0082, -0.0446, -0.0077, -0.1131]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0409,  0.0285,  0.0568, -0.0539, -0.0442],\n",
      "          [ 0.0500, -0.1732,  0.0954,  0.0636,  0.0071],\n",
      "          [-0.0860,  0.0095, -0.0713,  0.0070, -0.0161],\n",
      "          [-0.0905, -0.0119, -0.0014,  0.1023, -0.0305],\n",
      "          [-0.0309,  0.1183,  0.0121,  0.0194, -0.0020]]],\n",
      "\n",
      "\n",
      "        [[[-0.0644,  0.0083,  0.0004, -0.0445, -0.0849],\n",
      "          [-0.0553,  0.0346,  0.0040,  0.0391,  0.1343],\n",
      "          [-0.0421,  0.0511, -0.0438, -0.0407,  0.0445],\n",
      "          [ 0.0096,  0.0351,  0.0017, -0.0255,  0.0027],\n",
      "          [-0.0377, -0.0538, -0.0270,  0.1169, -0.0581]]]])\n"
     ]
    }
   ],
   "source": [
    "print('conv1.weight.grad before backward')\n",
    "print(net.conv1.weight.grad,'\\n')\n",
    "\n",
    "print(net.conv1.weight.shape,'\\n')\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print('conv1.weight.grad after backward')\n",
    "print(net.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Stochastic Gradient Descent (SGD) update rule used is \n",
    "\n",
    "# weight = weight - learning_rate * gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward(retain_graph=1)\n",
    "optimizer.step()    # Does the update\n",
    "\n",
    "print(optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
