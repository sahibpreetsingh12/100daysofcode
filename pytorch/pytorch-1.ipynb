{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()# super().__init__()  is equal to  super(Net,self).__init__() where self is the object of \n",
    "        #child class which is used to access proxy objectof Parent class created by super method .\n",
    "        \n",
    "        \n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        #Default value of stride is (1,1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120,bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84,bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10,bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=Net()\n",
    "print(net)#this is printed because we have imported Module class of nn package and in this class \n",
    "# __repr__() method is overwritten in this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "<class '__main__.Net'>\n"
     ]
    }
   ],
   "source": [
    "print(type(net.conv1))\n",
    "print(type(net))\n",
    "\n",
    "#this shows how we have accessed conv1 object using net object of Net class\n",
    "#conv1 is object we defined in our __init__ function is instance of torch.nn.modules.conv.Conv2d class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=400, out_features=120, bias=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=120, out_features=84, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0075,  0.0335, -0.1391, -0.0149, -0.1629],\n",
       "          [-0.1959,  0.1182, -0.0761,  0.0762,  0.1352],\n",
       "          [-0.0243, -0.0360,  0.0667,  0.0257,  0.1274],\n",
       "          [-0.0624, -0.1421,  0.0845, -0.0670,  0.1104],\n",
       "          [ 0.1296, -0.0075, -0.0516, -0.1560, -0.0269]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1445,  0.1893,  0.1091, -0.0712, -0.1071],\n",
       "          [-0.1056,  0.1350,  0.1746,  0.1655, -0.0128],\n",
       "          [ 0.0967, -0.1624, -0.0745, -0.1596, -0.1071],\n",
       "          [ 0.1860,  0.1283, -0.0142,  0.1654,  0.1081],\n",
       "          [ 0.0328,  0.1840,  0.0240,  0.0701,  0.1680]]],\n",
       "\n",
       "\n",
       "        [[[-0.0452,  0.0271,  0.0260,  0.0120, -0.0750],\n",
       "          [ 0.1900, -0.1402,  0.1953, -0.0894, -0.0882],\n",
       "          [ 0.1345, -0.1941,  0.0107,  0.1984, -0.0967],\n",
       "          [ 0.0157,  0.0235, -0.0935,  0.0262,  0.0236],\n",
       "          [-0.0963,  0.0223, -0.1641, -0.0579,  0.1371]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0804,  0.0354,  0.0147, -0.1890,  0.1929],\n",
       "          [ 0.1634,  0.0276,  0.0623,  0.0183, -0.0009],\n",
       "          [ 0.1304,  0.1352,  0.0776,  0.1849,  0.0978],\n",
       "          [ 0.0643, -0.0269,  0.1816, -0.0494, -0.0340],\n",
       "          [-0.0722,  0.0939, -0.1727, -0.0367, -0.0272]]],\n",
       "\n",
       "\n",
       "        [[[-0.1728,  0.1813,  0.0024, -0.1798,  0.1067],\n",
       "          [-0.1071, -0.0212, -0.1744,  0.1438, -0.0176],\n",
       "          [-0.1044,  0.1387, -0.1908, -0.1674, -0.1276],\n",
       "          [-0.0066, -0.0900,  0.0014, -0.0641,  0.0163],\n",
       "          [ 0.1114,  0.0865, -0.0716,  0.1734, -0.0252]]],\n",
       "\n",
       "\n",
       "        [[[-0.1152, -0.1742,  0.1512,  0.1448, -0.0607],\n",
       "          [ 0.0034,  0.0666,  0.0273, -0.0502,  0.0433],\n",
       "          [ 0.1257, -0.1022, -0.1314, -0.0947, -0.0453],\n",
       "          [ 0.0901, -0.1771,  0.1722, -0.1660,  0.0140],\n",
       "          [ 0.0979,  0.0894, -0.1408, -0.1127,  0.1415]]]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.weight\n",
    "#this shows how we can access weight object inside conv1 object \n",
    "\n",
    "#In output Below Parameter containing: means \n",
    "#the tensor printed is a special tensor which will learn the parameter values as we train our model\n",
    "#adn will try to achieve value in suchway that loss function is minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.conv1.weight.shape,'\\n')\n",
    "for i in range(net.conv1.weight.shape[0]):\n",
    "    print(net.conv1.weight[i].shape)#accessing a specific filter from six filters\n",
    "type(net.conv1.weight.shape)\n",
    "#output shows number of output channels,no of input channels,height of kernel,width of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.conv1.weight)\n",
    "#this shows that weight attribute belongs to Parameter class which is accessed by conv1 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "param=list(net.parameters())\n",
    "print(len(param))\n",
    "for param in net.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This shows that although we have defined :-\n",
    "## 5 layers(2 convolution layers,3 fully connected layers) \n",
    "## We have got 10 parameters because \n",
    "## By default for each layer we have Weight tensor and Bias Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight  SHAPE --->  torch.Size([6, 1, 5, 5]) \n",
      "\n",
      "conv1.bias  SHAPE --->  torch.Size([6]) \n",
      "\n",
      "conv2.weight  SHAPE --->  torch.Size([16, 6, 5, 5]) \n",
      "\n",
      "conv2.bias  SHAPE --->  torch.Size([16]) \n",
      "\n",
      "fc1.weight  SHAPE --->  torch.Size([120, 400]) \n",
      "\n",
      "fc1.bias  SHAPE --->  torch.Size([120]) \n",
      "\n",
      "fc2.weight  SHAPE --->  torch.Size([84, 120]) \n",
      "\n",
      "fc2.bias  SHAPE --->  torch.Size([84]) \n",
      "\n",
      "fc3.weight  SHAPE --->  torch.Size([10, 84]) \n",
      "\n",
      "fc3.bias  SHAPE --->  torch.Size([10]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name , param in net.named_parameters():\n",
    "    print(name ,\" SHAPE ---> \", param.shape,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1191, -0.6887,  0.5452])\n",
      "tensor([0.0000, 0.0000, 0.5452])\n"
     ]
    }
   ],
   "source": [
    "input=torch.randn(3)\n",
    "print(input)\n",
    "m = nn.functional.relu(input)\n",
    "print(m)\n",
    "\n",
    "#Rectifying Negative Input\n",
    "#This also shows Relu Does not Reduce the size of Input  tensor given to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0;34m\"\"\"shortcut for returning metadata with shape information, if defined\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m         \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0;34m\"\"\"shortcut for returning metadata with shape information, if defined\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m         \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"pytorch/images/relu.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([1, 1, 32, 32]) \n",
      "\n",
      "torch.Size([1, 6, 28, 28]) \n",
      "\n",
      "torch.Size([1, 6, 14, 14]) \n",
      "\n",
      "torch.Size([1, 16, 5, 5]) \n",
      "\n",
      "torch.Size([1, 400]) \n",
      "\n",
      "torch.Size([1, 120]) \n",
      "\n",
      "torch.Size([1, 84]) \n",
      "\n",
      "torch.Size([1, 10]) \n",
      "\n",
      "tensor([[ 0.0609,  0.0825, -0.1024, -0.0584, -0.0075,  0.0338,  0.0135, -0.0606,\n",
      "          0.1483, -0.1160]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        #Activation Functions do not reduce the size of Filter Map Given to Them as Input\n",
    "        relu_layer_1=F.relu(self.conv1(x))# Activation Relu is applied on First Convolutional Layer(conv1)\n",
    "        print(relu_layer_1.size(),'\\n')#Size has reduced from 32,32 in input to 28,28 after this RELU operation \n",
    "        #beacuse Input is First passed from convolution Layer then Relu Layer\n",
    "        \n",
    "\n",
    "\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(relu_layer_1, (2, 2))# Max Pooling is applied on output of Previous Relu Layer using 2,2 Max Pooling\n",
    "        # If the size is a square you can only specify a single number\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        print(x.size(),'\\n')\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        print(x.size(),'\\n')\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()#Setting Gradient to Zero to Deal with Problem of Accumulating Gradient\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32]) \n",
      "\n",
      "torch.Size([1, 6, 28, 28]) \n",
      "\n",
      "torch.Size([1, 6, 14, 14]) \n",
      "\n",
      "torch.Size([1, 16, 5, 5]) \n",
      "\n",
      "torch.Size([1, 400]) \n",
      "\n",
      "torch.Size([1, 120]) \n",
      "\n",
      "torch.Size([1, 84]) \n",
      "\n",
      "torch.Size([1, 10]) \n",
      "\n",
      "tensor([ 1.1264, -0.6373,  0.0545, -0.8404,  0.6918,  2.2573, -0.5519,  0.6574,\n",
      "        -0.5386, -0.3174])\n",
      "tensor([[ 1.1264, -0.6373,  0.0545, -0.8404,  0.6918,  2.2573, -0.5519,  0.6574,\n",
      "         -0.5386, -0.3174]])\n",
      "<class 'torch.nn.modules.loss.MSELoss'>\n",
      "tensor(0.9070, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "print(target)\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "print(target)\n",
    "criterion = nn.MSELoss()\n",
    "print(type(criterion))\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7fbb1164ab38>\n",
      "<AddmmBackward object at 0x7fbb1164aac8>\n",
      "<AccumulateGrad object at 0x7fbb1164ab38>\n",
      "<AccumulateGrad object at 0x7fbb1164aac8>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Print whole Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <MseLossBackward object at 0x7fbb1164c908> \n",
      "\n",
      "**** <AddmmBackward object at 0x7fbb1164ad30> \n",
      "\n",
      "******** <AccumulateGrad object at 0x7fbb1164a748> \n",
      "\n",
      "******** <ReluBackward0 object at 0x7fbb1164ab38> \n",
      "\n",
      "************ <AddmmBackward object at 0x7fbb1164c518> \n",
      "\n",
      "**************** <AccumulateGrad object at 0x7fbb1164cba8> \n",
      "\n",
      "**************** <ReluBackward0 object at 0x7fbb1164acc0> \n",
      "\n",
      "******************** <AddmmBackward object at 0x7fbb1164a7b8> \n",
      "\n",
      "************************ <AccumulateGrad object at 0x7fbb1164a978> \n",
      "\n",
      "************************ <ViewBackward object at 0x7fbb1164a898> \n",
      "\n",
      "**************************** <MaxPool2DWithIndicesBackward object at 0x7fbb1164a3c8> \n",
      "\n",
      "******************************** <ReluBackward0 object at 0x7fbb1164aa58> \n",
      "\n",
      "************************************ <MkldnnConvolutionBackward object at 0x7fbb1164a5f8> \n",
      "\n",
      "**************************************** <MaxPool2DWithIndicesBackward object at 0x7fbb1164ae80> \n",
      "\n",
      "******************************************** <ReluBackward0 object at 0x7fbb1164a198> \n",
      "\n",
      "************************************************ <MkldnnConvolutionBackward object at 0x7fbb1164a278> \n",
      "\n",
      "**************************************************** <AccumulateGrad object at 0x7fbb1164ac18> \n",
      "\n",
      "**************************************************** <AccumulateGrad object at 0x7fbb1164add8> \n",
      "\n",
      "**************************************** <AccumulateGrad object at 0x7fbb1164afd0> \n",
      "\n",
      "**************************************** <AccumulateGrad object at 0x7fbb1164acf8> \n",
      "\n",
      "************************ <TBackward object at 0x7fbb1164a358> \n",
      "\n",
      "**************************** <AccumulateGrad object at 0x7fbb1164a3c8> \n",
      "\n",
      "**************** <TBackward object at 0x7fbb1164aac8> \n",
      "\n",
      "******************** <AccumulateGrad object at 0x7fbb1164a7b8> \n",
      "\n",
      "******** <TBackward object at 0x7fbb1164a908> \n",
      "\n",
      "************ <AccumulateGrad object at 0x7fbb1164c518> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_graph(g, level=0):\n",
    "    if g == None: return\n",
    "    print('*'*level*4, g,'\\n')\n",
    "    for subg in g.next_functions:\n",
    "        print_graph(subg[0], level+1)\n",
    "\n",
    "print_graph(loss.grad_fn, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
